{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN3050/IN4050 Mandatory Assignment 3: Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:**\n",
    "\n",
    "**Username:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rules\n",
    "\n",
    "Before you begin the exercise, review the rules at this website: https://www.uio.no/english/studies/examinations/compulsory-activities/mn-ifi-mandatory.html , in particular the paragraph on cooperation. This is an individual assignment. You are not allowed to deliver together or copy/share source-code/answers with others. Read also the \"Routines for handling suspicion of cheating and attempted cheating at the University of Oslo\": https://www.uio.no/english/studies/examinations/cheating/index.html \n",
    "We do not entirely prohibit the use of generative language models (\"smart assistants\" like ChatGPT, Llama, Claude or Copilot), but you must clearly acknowledge this at all times, following the UiO guidelines: https://www.uio.no/english/studies/resources/ai_student.html\n",
    "Note also that you must fully understand _all_ the parts of you submissions, even if you got some help from a generative model. This will be tested during your peer review sessions (https://www.uio.no/studier/emner/matnat/ifi/IN3050/v25/Peer%20review/).\n",
    "By submitting this assignment, you confirm that you are familiar with the rules and the consequences of breaking them.\n",
    "\n",
    "### Delivery\n",
    "\n",
    "**Deadline**: Tuesday, April 22, 2025, 23:59 CEST\n",
    "\n",
    "Your submission should be delivered in Devilry. You may make several submissions in Devilry before the deadline, but only the last delivery will be read, so make sure to include all files in the last delivery. It is recommended to upload preliminary versions hours (or days) before the final deadline.\n",
    "\n",
    "### What to deliver?\n",
    "\n",
    "We recommend you to solve all exercises in this Jupyter notebook (alternative 1), but it is also possible to work with a regular Python script (alternative 2).\n",
    "\n",
    "**Alternative 1:** If you choose Jupyter, you should deliver the notebook. You should answer all questions and explain what you are doing in Markdown. Still, the code should be properly commented. The notebook should contain results of your runs. In addition, you also have to submit a PDF of your solution which shows the results of the runs.\n",
    "\n",
    "**Alternative 2:** If you prefer not to use notebooks, you should deliver the code as Python scripts together with your run results and a PDF report where you answer all the questions and explain your work.\n",
    "\n",
    "Here is a list of *absolutely necessary* (but not sufficient) conditions to get the assignment marked as passed:\n",
    "\n",
    "- You must deliver your code (Python script or Jupyter notebook) you used to solve the assignment.\n",
    "- The code used for making the output and plots must be included in the assignment. \n",
    "- You must include example runs that clearly shows how to run all implemented functions and methods.\n",
    "- All the code (in notebook cells or python main-blocks) must be runnable. If you have unfinished code that crashes, please comment it out and document what you think causes it to crash. \n",
    "- You must also deliver a PDF of the code, outputs, comments and plots as explained above.\n",
    "\n",
    "Make sure to include your name and username in the submitted files. Deliver one single compressed folder (.zip, .tgz or .tar.gz) which contains your complete solution.\n",
    "\n",
    "Important: if you weren’t able to finish the assignment, use the PDF report/Markdown to elaborate on what you’ve tried and what problems you encountered. Students who have made an effort and attempted all parts of the assignment will get a second chance even if they fail initially. This exercise will be graded PASS/FAIL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals of the assignment\n",
    "This assignment has two parts:\n",
    "1. You will go through the basic theory of **Principal Component Analysis (PCA)** and implement PCA from scratch to compress and visualize data.\n",
    "2. You will run **K-means clustering** using the `scikit-learn` toolkit and use PCA to visualize the results. You will also evaluate the output of K-means using a multi-class logistic regression classifier.\n",
    "\n",
    "IN4050 students will have to do one extra part about tuning PCA to balance compression with information lost.\n",
    "\n",
    "### Tools\n",
    "You may freely use code from the weekly exercises and the published solutions. In the first part about PCA you may **NOT** use ML libraries like `scikit-learn`. In the K-means part and beyond we encourage the use of `scikit-learn` to iterate quickly on the problems.\n",
    "\n",
    "We will use the *numpy* library for performing matrix computations and the *pyplot* library for plotting data, as well as *scikit-learn* for K-means clustering. This assignment also comes with a module called *data_assignment3* that you will use to import different (synthetic and real) datasets. Let's start by making sure that everything is installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy matplotlib scikit-learn\n",
    "\n",
    "import data_assignment3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Principal Component Analysis (PCA)\n",
    "In this section, you will work with the PCA algorithm in order to understand its definition and explore its uses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.1: Implementation\n",
    "Here we implement the basic steps of PCA and we assemble them. We will only need functions from *numpy* for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centering the Data\n",
    "Implement a function with the following signature to center the data. Remember that every *feature* should be centered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_data(A):\n",
    "    # INPUT:\n",
    "    # A    [NxM] numpy data matrix (N samples, M features)\n",
    "    #\n",
    "    # OUTPUT:\n",
    "    # X    [NxM] numpy centered data matrix (N samples, M features)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function checking the following assertion on *testcase* (absence of output means that the assertion holds):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcase = np.array([[3., 11., 4.3], [4., 5., 4.3], [5., 17., 4.5], [4, 13., 4.4]])\n",
    "answer = np.array([[-1., -0.5, -0.075], [0., -6.5, -0.075], [1., 5.5, 0.125], [0., 1.5, 0.025]])\n",
    "np.testing.assert_array_almost_equal(center_data(testcase), answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Covariance Matrix\n",
    "\n",
    "Implement a function with the following signature to compute the covariance matrix. In order to get this at the correct scale, divide by $N - 1$, not $N$.\n",
    "\n",
    "**Note:** Numpy provides a function `np.cov()` that does exactly this, but in this exercise we ask you to implement the code from scratch without using this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_covariance_matrix(A):\n",
    "    # INPUT:\n",
    "    # A    [NxM] numpy data matrix (N samples, M features)\n",
    "    #\n",
    "    # OUTPUT:\n",
    "    # C    [MxM] numpy covariance matrix (M features, M features)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function by comparing its output to the output of `np.cov()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_array = np.array([[22.,11.,5.5],[10.,5.,2.5],[34.,17.,8.5],[28.,14.,7]])\n",
    "answer = np.cov(np.transpose(test_array))\n",
    "to_test = compute_covariance_matrix(test_array)\n",
    "np.testing.assert_array_almost_equal(to_test, answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing eigenvalues and eigenvectors\n",
    "Use the linear algebra package of `numpy` and its function `np.linalg.eig()` to compute eigenvalues and eigenvectors. Note that we only take the real part of the eigenvectors and eigenvalues. The covariance matrix *should* be a symmetric matrix, but the actual implementation in `compute_covariance_matrix()` can lead to small round off errors that lead to tiny imaginary additions to the eigenvalues and eigenvectors. These are purely numerical artifacts that we can safely remove.\n",
    "\n",
    "**Note:** If you decide to NOT use `np.linalg.eig()` you must make sure that the eigenvalues you compute are of unit length!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_eigenvalue_eigenvectors(A):\n",
    "    # INPUT:\n",
    "    # A    [DxD] numpy matrix\n",
    "    #\n",
    "    # OUTPUT:\n",
    "    # eigval    [D] numpy vector of eigenvalues\n",
    "    # eigvec    [DxD] numpy array of eigenvectors\n",
    "    \n",
    "    eigval, eigvec = None, None\n",
    "    \n",
    "    # Numerical roundoff can lead to (tiny) imaginary parts. We correct that here.\n",
    "    eigval = eigval.real\n",
    "    eigvec = eigvec.real\n",
    "    \n",
    "    return eigval, eigvec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function checking the following assertion on *testcase*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcase = np.array([[2, 0, 0], [0, 5, 0], [0, 0, 4]])\n",
    "answer_eigval = np.array([2., 5., 4.])\n",
    "answer_eigvec = np.array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])\n",
    "x, y = compute_eigenvalue_eigenvectors(testcase)\n",
    "np.testing.assert_array_almost_equal(x, answer_eigval)\n",
    "np.testing.assert_array_almost_equal(y, answer_eigvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting eigenvalues and eigenvectors\n",
    "Implement a function with the following signature to sort eigenvalues and eigenvectors in descending order.\n",
    "\n",
    "Remember that eigenvalue `eigval[i]` corresponds to eigenvector `eigvec[:, i]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_eigenvalue_eigenvectors(eigval, eigvec):\n",
    "    # INPUT:\n",
    "    # eigval    [D] numpy vector of eigenvalues\n",
    "    # eigvec    [DxD] numpy array of eigenvectors\n",
    "    #\n",
    "    # OUTPUT:\n",
    "    # sorted_eigval    [D] numpy vector of eigenvalues\n",
    "    # sorted_eigvec    [DxD] numpy array of eigenvectors\n",
    "    \n",
    "    sorted_eigval, sorted_eigvec = None, None\n",
    "    return sorted_eigval, sorted_eigvec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function checking the following assertion on *testcase*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testcase = np.array([[2, 0, 0], [0, 5, 0], [0, 0, 4]])\n",
    "answer_eigval = np.array([5., 4., 2.])\n",
    "answer_eigvec = np.array([[0., 0., 1.], [1., 0., 0.], [0., 1., 0.]])\n",
    "x, y = compute_eigenvalue_eigenvectors(testcase)\n",
    "x, y = sort_eigenvalue_eigenvectors(x, y)\n",
    "np.testing.assert_array_almost_equal(x, answer_eigval)\n",
    "np.testing.assert_array_almost_equal(y, answer_eigvec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA Algorithm\n",
    "Implement a function with the following signature to compute PCA using the functions implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(A, m):\n",
    "    # INPUT:\n",
    "    # A    [NxM] numpy data matrix (N samples, M features)\n",
    "    # m    integer number denoting the number of learned features (m <= M)\n",
    "    #\n",
    "    # OUTPUT:\n",
    "    # pca_eigvec    [Mxm] numpy matrix containing the eigenvectors (M dimensions, m eigenvectors)\n",
    "    # P             [Nxm] numpy PCA data matrix (N samples, m features)\n",
    "\n",
    "    pca_eigvec, P = None, None\n",
    "    return pca_eigvec, P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your function checking the following assertion on *testcase*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "testcase = np.array([[22., 11., 5.5], [10., 5., 2.5], [34., 17., 8.5]])\n",
    "x, y = pca(testcase, 2)\n",
    "\n",
    "answer1 = pickle.load(open('PCAanswer1.pkl', 'rb'))\n",
    "answer2 = pickle.load(open('PCAanswer2.pkl', 'rb'))\n",
    "\n",
    "test_arr_x = np.sum(np.abs(np.abs(x) - np.abs(answer1)), axis=0)\n",
    "np.testing.assert_array_almost_equal(test_arr_x, np.zeros(2))\n",
    "\n",
    "test_arr_y = np.sum(np.abs(np.abs(y) - np.abs(answer2)))\n",
    "np.testing.assert_almost_equal(test_arr_y, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.2: Understanding - How does PCA work?\n",
    "We now use the PCA algorithm you implemented on a toy data set in order to understand its inner workings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "The module *data_assignment3* provides a small synthetic dataset of dimension [100x2] (100 samples, 2 features):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_assignment3\n",
    "\n",
    "X = data_assignment3.get_synthetic_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the data\n",
    "Visualize the synthetic data using the function *scatter()* from the *matplotlib* library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the centered data\n",
    "Notice that the data visualized above is not centered on the origin (0,0). Use the function defined above to center the data, and the replot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the first eigenvector\n",
    "Visualize the vector defined by the first eigenvector.\n",
    "To do this you need:\n",
    "- Use the *PCA()* function to recover the eigenvectors\n",
    "- Plot the centered data as done above \n",
    "- The first eigenvector is a 2D vector (x0,y0). This defines a vector with origin in (0,0) and head in (x0,y0). Use the function *plot()* from matplotlib to plot a line over the first eigenvector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_eigvec = None\n",
    "first_eigvec = None\n",
    "\n",
    "plt.scatter(None, None)\n",
    "\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "y = first_eigvec[1]/first_eigvec[0] * x\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the PCA projection\n",
    "Finally, use the *PCA()* algorithm to project on a single dimension and visualize the result using again the *scatter()* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.3: Evaluation - When are the results of PCA sensible?\n",
    "So far we have used PCA on synthetic data. Let us now imagine we are using PCA as a pre-processing step before a classification task. This is a common setup with high-dimensional data. We explore when the use of PCA is sensible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the first set of labels\n",
    "The function *get_synthetic_labeled_data_1()* from the module *data_assignment3* provides a first labeled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data_assignment3.get_synthetic_labeled_data_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running PCA\n",
    "Process the data using the PCA algorithm and project it in one dimension. Plot the labeled data using *scatter()* before and after running PCA. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before PCA\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y[:, 0])\n",
    "\n",
    "# after PCA\n",
    "plt.figure()\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** Add your comment here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the second set of labels\n",
    "The function *get_synthetic_labeled_data_2()* from the module *data_assignment3* provides a second labeled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data_assignment3.get_synthetic_labeled_data_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running PCA\n",
    "As before, process the data using the PCA algorithm and project it in one dimension. Plot the labeled data using *scatter()* before and after running PCA. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** Add your comment here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would the result change if you were to consider only the second eigenvector?\n",
    "What about if you were to consider both eigenvectors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: Add your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.4: Case study 1 - PCA for visualization\n",
    "\n",
    "The [*iris* flower dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set) is one of the oldest and best known data collections used for machine learning. It consists of 50 samples from each of three species of iris flowers (*Iris setosa, Iris virginica and Iris versicolor*). Four features were measured from each sample: sepal length, sepal width, petal length and petal width, all in centimeters.\n",
    "\n",
    "Visualizing a 4-dimensional dataset is impossible; therefore we will use PCA to project our data in 2 dimensions and visualize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "The function *get_iris_data()* from the module *data_assignment3* returns the *iris* dataset. It returns a data matrix of dimension [150x4] and a label vector of dimension [150]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data_assignment3.get_iris_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the data by selecting features\n",
    "Try to visualize the data (using label information) by randomly selecting two out of the four features of the data. You may try different pairs of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the data by PCA\n",
    "Process the data using PCA and visualize it (using label information). Compare with the previous visualization and comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** Enter your comment here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.5: Case study 2 - PCA for compression\n",
    "We now consider the *faces in the wild (lfw)* dataset, a collection of pictures (N=1280) of people. Each pixel in the image is a feature (M=2914)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "The function *get_lfw_data()* from the module *data_assignment3* returns the *lfw* dataset. It returns a data matrix of dimension [1280x2914] and a label vector of dimension [1280]. It also returns two parameters, $h$ and $w$, reporting the height and the width of the images (these parameters are necessary to plot the data samples as images). Beware, it might take some time to download the data. Be patient :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, h, w = data_assignment3.get_lfw_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the data\n",
    "Choose one datapoint to visualize (first coordinate of the matrix $X$) and use the function [imshow()](https://matplotlib.org/3.2.1/api/_as_gen/matplotlib.pyplot.imshow.html) to plot and inspect some of the pictures.\n",
    "\n",
    "Notice that the first argument of *imshow* is the image to be plotted; the image must be provided as a rectangular matrix, therefore we reshape a sample from the matrix $X$ to have height $h$ and width $w$. The parameter *cmap* specifies the color coding; in our case we will visualize the image in black-and-white with different gradations of grey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_number = 4    # display the 4th image of the collection\n",
    "plt.imshow(X[image_number, :].reshape((h, w)), cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a compression-decompression function\n",
    "Implement a function that first uses PCA to project samples in low-dimensions, and then reconstruct the original image.\n",
    "\n",
    "*Hint:* Most of the code is the same as the previous PCA() function you implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_decode_pca(A, m):\n",
    "    # INPUT:\n",
    "    # A    [NxM] numpy data matrix (N samples, M features)\n",
    "    # m    integer number denoting the number of learned features (m <= M)\n",
    "    #\n",
    "    # OUTPUT:\n",
    "    # Ahat [NxM] numpy PCA reconstructed data matrix (N samples, M features)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compressing and decompressing the data\n",
    "Use the implemented function to encode and decode the data by projecting on a lower dimensional space of dimension 200 (m=200)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xhat = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the reconstructed data\n",
    "Use the function *imshow* to plot and compare original and reconstructed pictures. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** Enter your comment here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating different compressions\n",
    "Use the previous setup to generate compressed images using different values of low dimensions in the PCA algorithm (e.g.: 20, 100, 200, 500, 1000). Plot and comment on the results. You can use `plt.subplot(n_rows, n_cols, position)` and `plt.title(titlestring)` to get a nice plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** Enter your comment here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.6: PCA tuning (compulsory for Master students only)\n",
    "If we use PCA for compression or decompression, it may be not trivial to decide how many dimensions to keep. In this section we review a principled way to decide how many dimensions to keep.\n",
    "\n",
    "The number of dimensions to keep is the only *hyper-parameter* of PCA. A method designed to decide how many dimensions/eigenvectors is the *proportion of variance*:\n",
    "$$ \\textrm{POV}=\\frac{\\sum_{i=1}^{m}{\\lambda_{i}}}{\\sum_{j=1}^{M}{\\lambda_{j}}}, $$\n",
    "where $\\lambda$ are eigenvalues, $M$ is the dimensionality of the original data, and $m$ is the chosen lower dimensionality. \n",
    "\n",
    "Using the $POV$ formula we may select a number $m$ of dimensions/eigenvalues so that the proportion of variance is, for instance, equal to 95%.\n",
    "\n",
    "Implement a new PCA for encoding and decoding that receives in input not the number of dimensions for projection, but the amount of proportion of variance to be preserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_decode_pca_with_pov(A, p):\n",
    "    # INPUT:\n",
    "    # A    [NxM] numpy data matrix (N samples, M features)\n",
    "    # p    float number between 0 and 1 denoting the POV to be preserved\n",
    "    #\n",
    "    # OUTPUT:\n",
    "    # Ahat [NxM] numpy PCA reconstructed data matrix (N samples, M features)\n",
    "    # m    integer reporting the number of dimensions selected\n",
    "\n",
    "    Ahat = None\n",
    "    m = None\n",
    "    return Ahat, m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the `lfw` dataset again. Use the implemented function to encode and decode the data by projecting on a lower dimensional space such that `POV=0.95`. Use the function `imshow` to plot and compare original and reconstructed pictures. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, h, w = data_assignment3.get_lfw_data()\n",
    "\n",
    "Xhat, m = None, None\n",
    "print(\"Selected dimensions:\", m)\n",
    "\n",
    "# Plot the images here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** Enter your comment here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: K-Means Clustering\n",
    "In this section you will use the *k-means clustering* algorithm to perform unsupervised clustering. Then you will perform a qualitative assessment of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.1: Applying K-Means and Qualitative Assessment\n",
    "\n",
    "### Importing scikit-learn library\n",
    "We start importing the module `sklearn.cluster.KMeans` from the standard machine learning library `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "We will use once again the *iris* data set. Start by loading the dataset again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data_assignment3.get_iris_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projecting the data using PCA\n",
    "To allow for visualization, we project our data in two dimensions as we did previously. This step is not necessary, and we may want to try to use *k-means* later without the PCA pre-processing. But to start, we use PCA, as this will allow for an easy visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running k-means\n",
    "Use the class *KMeans* to fit and predict the output of the *k-means* algorithm on the projected data (note that we don't use the true labels `y` here). Run the algorithm using the following values of $k=\\{2,3,4,5\\}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [2, 3, 4, 5]\n",
    "y_hats = []\n",
    "\n",
    "for i in range(len(k_values)):\n",
    "    # Initialize KMeans object with correct value for k\n",
    "    # Fit the algorithm to the training data and store the predictions in y_hats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative assessment\n",
    "Plot the results of running the k-means algorithm, compare with the true labels, and comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** Enter your comment here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.2: Quantitative Assessment of K-Means\n",
    "\n",
    "Above, we used k-means for clustering and assessed the results qualitatively by visualizing them. However, we often want to measure in a quantitative way how good a clustering is. To do this, we will use a classification task to **evaluate numerically how good the learned clusters are for all the different values of k you used above (2 to 5)**.\n",
    "\n",
    "Informally, our evaluation will work as follows: For each of our clusterings ($k=2$ to $k=5$), we will try to learn a mapping from the identified clusters to the correct labels of our datapoints. The reason this can be a sensible evaluation, is that to learn a good mapping, we have to have identified clusters that correspond to the actual classes in our data. We will in other words train 4 different classification models (one for each $k$ value), where the input to our classifier is the cluster each datapoint belongs to, and the target is the correct class for this datapoint. In other words, **we aim to learn to classify datapoints as well as possible with the only information available to the classifier being the cluster that datapoint belongs to**. For some values of k, we will get poorly performing classifiers, indicating that this clustering has not revealed the correct class division in our data.\n",
    "\n",
    "In practice, you will do the following: Reload the *iris* dataset. Import a standard `LogisticRegression` classifier from the module `sklearn.linear_model`. Use the k-means representations learned previously (`yhats[2],...,yhats[5]`) and the true label to train the classifier. Evaluate your model on the training data (we do not have a test set, so this procedure will assess the model fit instead of generalization) using the `accuracy_score()` function from the `sklearn.metrics` module. Plot a graph showing how the accuracy score varies when changing the value of $k$. Comment on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train a Logistic regression model using the first two dimensions of the PCA of the iris data set as input, and the true classes as targets.\n",
    "- Report the model fit/accuracy on the training set.\n",
    "- For each value of K:\n",
    "  - One-Hot-Encode the classes outputed by the K-means algorithm.\n",
    "  - Train a Logistic regression model on the K-means classes as input vs the real classes as targets.\n",
    "  - Calculate model fit/accuracy vs. value of K.\n",
    "- Plot your results in a graph and comment on the K-means fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:** Add your comment here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions \n",
    "\n",
    "In this notebook we studied **unsupervised learning** considering two important and representative algorithms: **PCA** and **k-means**.\n",
    "\n",
    "First, we implemented the PCA algorithm step by step; we then ran the algorithm on synthetic data in order to see its working and evaluated when it makes sense to use it and when it doesn't. We then considered two typical uses of PCA: for **visualization** on the *iris* dataset, and for **compression-decompression** on the *lfw* dataset. We also looked at an additional question that arises when using PCA: the problem of **selection of hyper-parameters**, that is, how to select the optimal hyper-parameter of our algorithm for a particular task.\n",
    "\n",
    "We then moved to consider the k-means algorithm. In this case we used the implementation provided by *scikit-learn* and applied it to another prototypical unsupervised learning problem: **clustering**. We processed the *iris* dataset with *k-means* and evaluated the results visually. We also considered the problem of **quantitative evaluation** of the results, that is, how to measure the performance or usefulness of k-means clustering on a downstream task (classifying the *iris* samples into their species)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
