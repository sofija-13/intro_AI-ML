{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IN3050/IN4050 Mandatory Assignment 2, 2025: Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name: Sofija GRANET\n",
    "## Username: sofijag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rules\n",
    "\n",
    "Before you begin the exercise, review the rules at this website: https://www.uio.no/english/studies/examinations/compulsory-activities/mn-ifi-mandatory.html , in particular the paragraph on cooperation. This is an individual assignment. You are not allowed to deliver together or copy/share source-code/answers with others. Read also the \"Routines for handling suspicion of cheating and attempted cheating at the University of Oslo\": https://www.uio.no/english/studies/examinations/cheating/index.html \n",
    "We do not entirely prohibit the use of generative language models (\"smart assistants\" like ChatGPT, Llama, Claude or Copilot), but you must clearly acknowledge this at all times, following the UiO guidelines: https://www.uio.no/english/studies/resources/ai_student.html\n",
    "Note also that you must fully understand _all_ the parts of you submissions, even if you got some help from a generative model. This will be tested during your peer review sessions (https://www.uio.no/studier/emner/matnat/ifi/IN3050/v25/Peer%20review/).\n",
    "By submitting this assignment, you confirm that you are familiar with the rules and the consequences of breaking them.\n",
    "\n",
    "### Delivery\n",
    "\n",
    "**Deadline**: Friday, March 28, 2025, 23:59\n",
    "\n",
    "Your submission should be delivered in Devilry. You may redeliver in Devilry before the deadline, but include all files in the last delivery, as only the last delivery will be read. You are recommended to upload preliminary versions hours (or days) before the final deadline.\n",
    "\n",
    "### What to deliver?\n",
    "\n",
    "You are recommended to solve the exercise in a Jupyter notebook, but you might solve it in a regular Python script if you prefer.\n",
    "\n",
    "#### Alternative 1\n",
    "If you prefer not to use notebooks, you should deliver the code, your run results, and a PDF report where you answer all the questions and explain your work.\n",
    "\n",
    "#### Alternative 2\n",
    "If you choose Jupyter, you should deliver the notebook. You should answer all questions and explain what you are doing in Markdown. Still, the code should be properly commented. The notebook should contain results of your runs. In addition, you should make a PDF of your solution which shows the results of the runs. (If you can't export: notebook -> latex -> pdf on your own machine, you may do this on the IFI linux machines.)\n",
    "\n",
    "Here is a list of *absolutely necessary* (but not sufficient) conditions to get the assignment marked as passed:\n",
    "\n",
    "- You must deliver your code (Python script or Jupyter notebook) you used to solve the assignment.\n",
    "- The code used for making the output and plots must be included in the assignment. \n",
    "- You must include example runs that clearly shows how to run all implemented functions and methods.\n",
    "- All the code (in notebook cells or python main-blocks) must be runnable. If you have unfinished code that crashes, please comment it out and document what you think causes it to crash. \n",
    "- You must also deliver a PDF of the code, outputs, comments and plots as explained above.\n",
    "\n",
    "Your report/notebook should contain your name and username.\n",
    "\n",
    "Deliver one single compressed folder (.zip, .tgz or .tar.gz) which contains your complete solution.\n",
    "\n",
    "Important: if you weren’t able to finish the assignment, use the PDF report/Markdown to elaborate on what you’ve tried and what problems you encountered. Students who have made an effort and attempted all parts of the assignment will get a second chance even if they fail initially. This exercise will be graded PASS/FAIL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals of the assignment\n",
    "The goal of this assignment is to get a better understanding of supervised learning with gradient descent. It will, in particular, consider the similarities and differences between linear classifiers and multi-layer feed forward neural networks (multi-layer perceptrons, MLP) and the differences and similarities between binary and multi-class classification. \n",
    "\n",
    "### Tools\n",
    "The aim of the exercises is to give you a look inside the learning algorithms. You may freely use code from the weekly exercises and the published solutions. You should not use machine learning libraries like Scikit-Learn or PyTorch, because the point of this assignment is for you to implement things from scratch. You, however, are encouraged to use tools like NumPy, Pandas and MatPlotLib, which are not ML-specific.\n",
    "\n",
    "The given precode uses NumPy. You are recommended to use NumPy since it results in more compact code, but feel free to use pure Python if you prefer. \n",
    "\n",
    "If anything is unclear, do not hesitate to ask. Also, if you think some assumptions are missing, make your own and explain them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/__init__.py:84\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     81\u001b[0m         __check_build,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     82\u001b[0m         _distributor_init,  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     83\u001b[0m     )\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[1;32m     87\u001b[0m     __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     88\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_versions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    131\u001b[0m     ]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/base.py:19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/utils/__init__.py:11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _joblib, metadata_routing\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bunch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_chunking\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/utils/_chunking.py:8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[1;32m     12\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumbers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Integral, Real\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/scipy/__init__.py:86\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# This is the first import of an extension module within SciPy. If there's\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# a general issue with the install, such that extension modules are missing\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# or cannot be imported, this is where we'll get a failure - so give an\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# informative error message.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ccallback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LowLevelCallable\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     88\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `scipy` install you are using seems to be broken, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m     89\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(extension modules cannot be imported), \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m     90\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease try reinstalling.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/scipy/_lib/_ccallback.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _ccallback_c\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mctypes\u001b[39;00m\n\u001b[1;32m      5\u001b[0m PyCFuncPtr \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mCFUNCTYPE(ctypes\u001b[38;5;241m.\u001b[39mc_void_p)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__bases__\u001b[39m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:645\u001b[0m, in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn # This is only to generate a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "We start by making a synthetic dataset of 5000 instances and ten classes, with 500 instances in each class. (See https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html regarding how the data are generated.) We choose to use a synthetic dataset---and not a set of natural occuring data---because we are mostly interested in properties of the various learning algorithms, in particular the differences between linear classifiers and multi-layer neural networks together with the difference between binary and multi-class data. In addition, we would like a dataset with instances represented with only two numerical features, so that it is easy to visualize the data. It would be rather difficult (although not impossible) to find a real-world dataset of the same nature. Anyway, you surely can use the code in this assignment for training machine learning models on real-world datasets.\n",
    "\n",
    "When we are doing experiments in supervised learning, and the data are not already split into training and test sets, we should start by splitting the data. Sometimes there are natural ways to split the data, say training, on data from one year and testing on data from a later year, but if that is not the case, we should shuffle the data randomly before splitting. (OK, that is not necessary with this particular synthetic data set, since it is already shuffled by default by Scikit-Learn, but that will not be the case with real-world data) We should split the data so that we keep the alignment between X (features) and t (class labels), which may be achieved by shuffling the indices. We split into 60% for training, 20% for validation, and 20% for final testing. The set for final testing *must not be used* till the end of the assignment in part 3.\n",
    "\n",
    "We fix the seed both for data set generation and for shuffling, so that we work on the same datasets when we rerun the experiments. This is done by the `random_state` argument and the `rng = np.random.RandomState(424242)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "X, t_multi = make_blobs(n_samples=[500, 500, 500, 500, 500, 500, 500, 500, 500, 500], centers=[[0,1],[4,2],[8,1],[2,0],[6,0],[3,-3],[4,-2],[0,5],[0,4],[-2,-2]], \n",
    "                  n_features=2, random_state=424242, cluster_std=[1.0, 2.0, 1.0, 0.5, 0.5, 3.0, 1.0, 0.5, 2.5, 2.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffling the dataset\n",
    "indices = np.arange(X.shape[0])\n",
    "rng = np.random.RandomState(424242)\n",
    "rng.shuffle(indices)\n",
    "indices[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into train, dev and test\n",
    "X_train = X[indices[:3000],:]\n",
    "X_val = X[indices[3000:4000],:]\n",
    "X_test = X[indices[4000:],:]\n",
    "t_multi_train = t_multi[indices[:3000]]\n",
    "t_multi_val = t_multi[indices[3000:4000]]\n",
    "t_multi_test = t_multi[indices[4000:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will  make a second dataset with only two classes by merging the existing labels in (X,t), so that `0-5` become the new `0` and `6-9` become the new `1`. Let's call the new set (X, t2). This will be a binary set.\n",
    "We now have two datasets:\n",
    "\n",
    "- Binary set: `(X, t2)`\n",
    "- Multi-class set: `(X, t_multi)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2_train = t_multi_train >= 6\n",
    "t2_train = t2_train.astype(\"int\")\n",
    "t2_val = (t_multi_val >= 6).astype(\"int\")\n",
    "t2_test = (t_multi_test >= 6).astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the two traning sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6)) # You may adjust the size\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=t_multi_train, s=10.0)\n",
    "plt.title(\"Multi-class set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=t2_train, s=10.0)\n",
    "plt.title(\"Binary set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Linear classifiers\n",
    "### Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that even the binary set (X, t2) is far from linearly separable, and we will explore how various classifiers are able to handle this. We start with linear regression with the Mean Squared Error (MSE) loss, although it is not the most widely used approach for classification tasks: but we are interested. You may make your own implementation from scratch or start with the solution to the weekly exercise set 6. We include it here with a little added flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bias(X, bias):\n",
    "    \"\"\"X is a NxM matrix: N datapoints, M features\n",
    "    bias is a bias term, -1 or 1, or any other scalar. Use 0 for no bias\n",
    "    Return a Nx(M+1) matrix with added bias in position zero\n",
    "    \"\"\"\n",
    "    N = X.shape[0]\n",
    "    biases = np.ones((N, 1)) * bias # Make a N*1 matrix of biases\n",
    "    # Concatenate the column of biases in front of the columns of X.\n",
    "    return np.concatenate((biases, X), axis  = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyClassifier():\n",
    "    \"\"\"Common methods to all Numpy classifiers --- if any\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyLinRegClass(NumpyClassifier):\n",
    "\n",
    "    def __init__(self, bias=-1):\n",
    "        self.bias=bias\n",
    "    \n",
    "    def fit(self, X_train, t_train, lr = 0.1, epochs=10):\n",
    "        \"\"\"X_train is a NxM matrix, N data points, M features\n",
    "        t_train is avector of length N,\n",
    "        the target class values for the training data\n",
    "        lr is our learning rate\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.bias:\n",
    "            X_train = add_bias(X_train, self.bias)\n",
    "            \n",
    "        (N, M) = X_train.shape\n",
    "        \n",
    "        self.weights = weights = np.zeros(M)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # print(\"Epoch\", epoch)\n",
    "            weights -= lr / N *  X_train.T @ (X_train @ weights - t_train)      \n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        \"\"\"X is a KxM matrix for some K>=1\n",
    "        predict the value for each point in X\"\"\"\n",
    "        if self.bias:\n",
    "            X = add_bias(X, self.bias)\n",
    "        ys = X @ self.weights\n",
    "        return ys > threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can train and test a first classifier (on the binary dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predicted, gold):\n",
    "    return np.mean(predicted == gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl = NumpyLinRegClass()\n",
    "cl.fit(X_train, t2_train, epochs=10)\n",
    "accuracy(cl.predict(X_val), t2_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a small procedure which plots the data set together with the decision boundaries. \n",
    "You may modify the colors and the rest of the graphics as you like.\n",
    "The procedure will also work for multi-class classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_regions(X, t, clf=[], size=(8,6)):\n",
    "    \"\"\"Plot the data set (X,t) together with the decision boundary of the classifier clf\"\"\"\n",
    "    # The region of the plane to consider determined by X\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    \n",
    "    # Make a prediction of the whole region\n",
    "    h = 0.02  # step size in the mesh\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    # Classify each meshpoint.\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.figure(figsize=size) # You may adjust this\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    plt.contourf(xx, yy, Z, alpha=0.2, cmap = 'tab10')\n",
    "\n",
    "    plt.scatter(X[:,0], X[:,1], c=t, s=10.0, cmap='tab10')\n",
    "\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(\"Decision regions\")\n",
    "    plt.xlabel(\"x0\")\n",
    "    plt.ylabel(\"x1\")\n",
    "\n",
    "#    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_regions(X_train, t2_train, cl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Tuning\n",
    "\n",
    "The result is far from impressive. \n",
    "Remember that a classifier which always chooses the majority class will have an accuracy of 0.6 on this data set.\n",
    "\n",
    "Your task is to try various settings for the two training hyper-parameters, learning rate and the number of epochs, to get the best accuracy on the validation set. \n",
    "\n",
    "Report how the accuracy varies with the hyper-parameter settings. It it not sufficient to give the final hyperparameters. You must also show how you found then and results for alternative values you tried aout.\n",
    "\n",
    "When you are satisfied with the result, you may plot the decision boundaries, as above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen in the lectures that scaling the data may improve training speed and sometimes the performance. \n",
    "\n",
    "- Implement the standard scaler (normalizer); you can also try other scaling techniques\n",
    "- Scale the data\n",
    "- Train the model on the scaled data\n",
    "- Experiment with hyper-parameter settings and see whether you can speed  up or improve the training.\n",
    "- Report final hyper-parameter settings and show how you found them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "a) You should now implement a logistic regression classifier similarly to the classifier based on linear regression.\n",
    "You may use the code from the solution to weekly exercise set week06.\n",
    "\n",
    "b) In addition to the method `predict()` which predicts a class for the data, include a method `predict_probabilities()` which predicts the probability of the data belonging to the positive class.\n",
    "\n",
    "c) So far, we have not calculated the loss explicitly in the code. Extend the code to calculate the loss on the training set for each epoch and to store the losses such that the losses can be inspected after training. The prefered loss for logistic regression is binary cross-entropy, but you can also try mean squared error. The most important is that your implementation of the loss corresponds to your implementation of the gradient descent.\n",
    "Also, calculate and store accuracies after each epoch.\n",
    "\n",
    "d) In addition, extend the `fit()` method with optional arguments for a validation set (X_val, t_val). If a validation set is included in the call to `fit()`, calculate the loss and the accuracy for the validation set after each epoch. \n",
    "\n",
    "e) The training runs for a number of epochs. We cannot know beforehand for how many epochs it is reasonable to run the training. One possibility is to run the training until the learning does not improve much. Extend the `fit()` method with two keyword arguments, `tol` (tolerance) and `n_epochs_no_update` and stop training when the loss has not improved with more than `tol` after `n_epochs_no_update` (to save compute and potentially avoid over-fitting). A possible default value for `n_epochs_no_update` is 2. Also, add an attribute to the classifier which tells us after fitting how many epochs it was trained for.\n",
    "\n",
    "f) Train classifiers with various learning rates, and with varying values for `tol` for finding the optimal values. Also consider the effect of scaling the data.\n",
    "\n",
    "g) After a succesful training, for your best model, plot both training loss and validation loss as functions of the number of epochs in one figure, and both training and validation accuracies as functions of the number of epochs in another figure. Comment on what you see. Are the curves monotone? Is this as expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class classifiers\n",
    "We turn to the task of classifying when there are more than two classes, and the task is to ascribe one class to each input. We will now use the set `(X, t_multi)`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class with logistic regression\n",
    "We saw in the lectures how a logistic regression classifier can be turned into a multi-class classifier using the one-vs-rest approach. We train one logistic regression classifier for each class. To predict the class of an item, we run all the binary classifiers and collect the probability score from each of them. We assign the class which ascribes the highest probability.\n",
    "\n",
    "Build such a classifier. Train the resulting classifier on `(X_train, t_multi_train)`, test it on `(X_val, t_multi_val)`, tune the hyper-parameters and report the accuracy.\n",
    "\n",
    "Also plot the decision boundaries for your best classifier similarly to the plots for the binary case.\n",
    "\n",
    "### For IN4050 students: Multinomial logistic regression\n",
    "The following part is only mandatory for IN4050 students. IN3050 students are also welcome to make it a try. Everybody has to do the part 2 on multi-layer neural networks. \n",
    "\n",
    "In the lectures, we contrasted the one-vs-rest approach with the multinomial logistic regression, also called softmax classifier. Implement also this classifier, tune the parameters, and compare the results to the classifiers above. (Don't expect a large difference on a simple task like this.)\n",
    "\n",
    "Remember that this classifier uses softmax in the forward phase. For loss, it uses categorical cross-entropy loss. The loss has a somewhat simpler form than in the binary case. To calculate the gradient is a little more complicated. The actual gradient and update rule is simple, however, as long as you have calculated the forward values correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our classifier is not ideal. But is it because all the 10 classes are equally diffucult to predict?\n",
    "You should evaluate the multinomial model predictions for each class separately and report your findings.\n",
    "Please also report whether there is any difference in this respect between the training and the validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Multi-layer neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first non-linear classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is a simple implementation of a multi-layer perceptron or feed-forward neural network.\n",
    "For now, it is quite restricted.\n",
    "There is only one hidden layer with 6 neurons.\n",
    "It can only handle binary classification.\n",
    "In addition, it uses a simple final layer similar to the linear regression classifier above.\n",
    "One way to look at it is what happens when we add a hidden layer to the linear regression classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLP class below misses the implementation of the `forward()` function. Your first task is to implement it. \n",
    "\n",
    "Remember that in the forward pass, we \"feed\" the input to the model, the model processes it and produces the output. The function should make use of the logistic activation function and bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we define the logistic function and its derivative:\n",
    "def logistic(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def logistic_diff(y):\n",
    "    return y * (1 - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBinaryLinRegClass(NumpyClassifier):\n",
    "    \"\"\"A multi-layer neural network with one hidden layer\"\"\"\n",
    "    \n",
    "    def __init__(self, bias=-1, dim_hidden = 6):\n",
    "        \"\"\"Intialize the hyperparameters\"\"\"\n",
    "        self.bias = bias\n",
    "        # Dimensionality of the hidden layer\n",
    "        self.dim_hidden = dim_hidden\n",
    "        \n",
    "        self.activ = logistic\n",
    "        \n",
    "        self.activ_diff = logistic_diff\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"TODO: \n",
    "        Perform one forward step. \n",
    "        Return a pair consisting of the outputs of the hidden_layer\n",
    "        and the outputs on the final layer\"\"\"\n",
    "        \n",
    "        raise NotImplementedError\n",
    "        # return hidden_outs, outputs\n",
    "    \n",
    "    def fit(self, X_train, t_train, lr=0.001, epochs = 100):\n",
    "        \"\"\"Initialize the weights. Train *epochs* many epochs.\n",
    "        \n",
    "        X_train is a NxM matrix, N data points, M features\n",
    "        t_train is a vector of length N of targets values for the training data, \n",
    "        where the values are 0 or 1.\n",
    "        lr is the learning rate\n",
    "        \"\"\"\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Turn t_train into a column vector, a N*1 matrix:\n",
    "        T_train = t_train.reshape(-1,1)\n",
    "            \n",
    "        dim_in = X_train.shape[1] \n",
    "        dim_out = T_train.shape[1]\n",
    "        \n",
    "        # Initialize the weights\n",
    "        self.weights1 = (np.random.rand(\n",
    "            dim_in + 1, \n",
    "            self.dim_hidden) * 2 - 1)/np.sqrt(dim_in)\n",
    "        self.weights2 = (np.random.rand(\n",
    "            self.dim_hidden+1, \n",
    "            dim_out) * 2 - 1)/np.sqrt(self.dim_hidden)\n",
    "        X_train_bias = add_bias(X_train, self.bias)\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            # One epoch\n",
    "            # The forward step:\n",
    "            hidden_outs, outputs = self.forward(X_train_bias)\n",
    "            # The delta term on the output node:\n",
    "            out_deltas = (outputs - T_train)\n",
    "            # The delta terms at the output of the hidden layer:\n",
    "            hiddenout_diffs = out_deltas @ self.weights2.T\n",
    "            # The deltas at the input to the hidden layer:\n",
    "            hiddenact_deltas = (hiddenout_diffs[:, 1:] * \n",
    "                                self.activ_diff(hidden_outs[:, 1:]))  \n",
    "\n",
    "            # Update the weights:\n",
    "            self.weights2 -= self.lr * hidden_outs.T @ out_deltas\n",
    "            self.weights1 -= self.lr * X_train_bias.T @ hiddenact_deltas \n",
    "            \n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict the class for the members of X\"\"\"\n",
    "        Z = add_bias(X, self.bias)\n",
    "        forw = self.forward(Z)[1]\n",
    "        score= forw[:, 0]\n",
    "        return (score > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When implemented, this model can be used to make a non-linear classifier for the set `(X, t2)`. Experiment with settings for learning rate and epochs and see how good results you can get. \n",
    "Report results for various settings. Be prepared to train for a long time (but you can control it via the number of epochs and hidden size). \n",
    "\n",
    "Plot the training set together with the decision regions as in Part I."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving the MLP classifier\n",
    "You should now make changes to the classifier similarly to what you did with the logistic regression classifier in part 1.\n",
    "\n",
    "a) In addition to the `predict()` method, which predicts a class for the data, include the `predict_probabilities()` method which predict the probability of the data belonging to the positive class. The training should be based on these values, as with logistic regression.\n",
    "\n",
    "b) Calculate the loss and the accuracy after each epoch and store them for inspection after training.\n",
    "\n",
    "c) Extend the `fit()` method with optional arguments for a validation set `(X_val, t_val)`. If a validation set is included in the call to `fit()`, calculate the loss and the accuracy for the validation set after each epoch.\n",
    "\n",
    "d) Extend the `fit()` method with two keyword arguments, `tol` (tolerance) and `n_epochs_no_update` and stop training when the loss has not improved for more than `tol` after `n_epochs_no_update`. A possible default value for `n_epochs_no_update` is 2. Add an attribute to the classifier which tells us after fitting how many epochs it was trained on.\n",
    "\n",
    "e) Tune the hyper-parameters: `lr`, `tol` and `dim-hidden` (size of the hidden layer).\n",
    "Also, consider the effect of scaling the data.\n",
    "\n",
    "f) After a succesful training with the best setting for the hyper-parameters, plot both training loss and validation loss as functions of the number of epochs in one figure, and both training and validation accuracies as functions of the number of epochs in another figure. Comment on what you see.\n",
    "\n",
    "g) The MLP algorithm contains an element of non-determinism. Hence, train the classifier 3 times with the optimal hyper-parameters and report the mean and standard deviation of the accuracies over the 3 runs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For IN4050-students: Multi-class neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following part is only mandatory for IN4050 students. IN3050 students are also welcome to make it a try. This is the most fun part of the set :) )\n",
    "\n",
    "The goal is to use a feed-forward neural network for non-linear multi-class classfication and apply it to the set `(X, t_multi)`.\n",
    "\n",
    "Modify the network to become a multi-class multinomial classifier. As a sanity check of your implementation, you may apply it to `(X, t_2)` and see whether you get similar results as above.\n",
    "\n",
    "Train the resulting classifier on `(X_train, t_multi_train)`, test it on `(X_val, t_multi_val)`, tune the hyper-parameters and report the accuracy.\n",
    "\n",
    "Plot the decision boundaries for your best classifier. Evaluate the  best model predictions for each class separately (same as in part 1)  and report your findings. Please also report whether there is any difference in this respect between the training and the validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Final testing\n",
    "We can now perform a final testing on the held-out test set we created in the beginning.\n",
    "\n",
    "## Binary task (X, t2)\n",
    "Consider the linear regression classifier, the logistic regression classifier and the multi-layer network with the best settings you found. Train each of them on the training set and evaluate on the held-out test set, but also on the validation set and the training set. Report the performance in a 3 by 3 table.\n",
    "\n",
    "Comment on what you see. How do the three different algorithms compare? Also, compare the results between the different dataset splits. In cases like these, one might expect slightly inferior results on the held-out test data compared to the validation and training data. Is this the case? \n",
    "\n",
    "Also report precision and recall for class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For IN4050-students: Multi-class task (X, t_multi)\n",
    "\n",
    "The following part is only mandatory for IN4050-students. IN3050-students are also welcome to give it a try though.\n",
    "\n",
    "Compare the three multi-class classifiers: the standard multi-class and the multinomial logistic regression from part one and the multi-class neural network from part two. Evaluate on test, validation and training set as above.\n",
    "\n",
    "Comment on the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
